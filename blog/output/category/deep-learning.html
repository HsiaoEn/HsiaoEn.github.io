<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>HsiaoEn's Blog - deep learning</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">HsiaoEn's Blog</a></h1>
                <nav><ul>
                    <li><a href="/pages/welcome-to-pelican.html">Welcome to Pelican!</a></li>
                    <li class="active"><a href="/category/deep-learning.html">deep learning</a></li>
                    <li><a href="/category/python.html">Python</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/2022/intro-to-attention-mechanism.html">Intro to Attention Mechanism</a></h1>
<footer class="post-info">
        <abbr class="published" title="2022-02-03T22:08:00+08:00">
                Published: 2022/2/3
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/hsiaoen.html">HsiaoEn</a>
        </address>
<p>In <a href="/category/deep-learning.html">deep learning</a>.</p>
<p>tags: <a href="/tag/attention.html">attention</a> <a href="/tag/rnn.html">rnn</a> <a href="/tag/seq2seq.html">seq2seq</a> </p>
</footer><!-- /.post-info --><p>注意力機制(Attention Mechanism)是現今自然語言處理(Natural Language Processing, NLP)領域發展當中重要的一環，<strong>它幫助解決了 RNN 本身記憶力受限的問題</strong>，這樣的機制使得模型在原有 NLP 的任務上表現更好。若是要從一開始了解 Attention 的概念，應當從 Seq2Seq Model 說起，因此下面將以 Seq2Seq 做為起始的說明，接著再看到 Attention 如何被帶到 Seq2Seq 的模型之中，最後再來看看關於 Attention 在問答模型(QA)中的應用，以此來比較在不同情境中 Attention 可以如何被利用，以下介紹將基於下面所列的參考文獻做為說明基礎：</p>
<ul>
<li>Seq2Seq Model:<ul>
<li>[Sutskever, et al. 2014] <a href="https://arxiv.org/abs/1409.3215"><strong><em>Sequence to Sequence Learning with Neural Networks</em></strong></a></li>
<li>[KyungHyun Cho, et al. 2014] <a href="https://arxiv.org/abs/1406.1078"><strong><em>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em></strong></a></li>
</ul>
</li>
<li>Basic Attention Mechanism:<ul>
<li>[Dzmitry Bahdanau, et al. 2015] <a href="https://arxiv.org/abs/1409.0473"><strong><em>Neural Machine Translation by Jointly Learning to Align and Translate</em></strong></a></li>
</ul>
</li>
<li>Attention applied in QA Problem<ul>
<li>[Cicero dos Santos, et al. 2016] <a href="https://arxiv.org/abs/1602.03609"><strong><em>Attentive Pooling Networks</em></strong></a></li>
</ul>
</li>
</ul>
<h2>Seq2Seq Model</h2>
<p>我們日常用來溝通的語言，例如「你好嗎？」、「今天天氣不錯！」, 可以被看待成一連串的字詞依照出現時間先後順序所做的排列，也就是時間序列(Time series)。在 NLP 領域中，有一類任務所面對的問題是輸入是一串序列(Input Sequence)：<span class="math">\(x_1, x_2, ..., x_S\)</span>，目標的輸出也是一串序列(Target Sequence)：<span class="math">\(y_1, y_2, ..., y_T\)</span>, 而且特別要注意的是這兩個 Sequence 的長度可以不一樣長，例如機器翻譯(Machine Translation, MT)就屬這類的任務之一，對於這樣輸入是 sequence，輸出也是 sequence 的模型我們稱為 Sequence to Sequence Model，或簡寫為 Seq2Seq Model:</p>
<p><img src="../images/intro_to_attention_01.png" alt="未顯示圖片" height="40%" width="40%"></p>
<p><em>Fig. 1. Seq2Seq Model</em>
<br></p>
<p>首先, 對於 Seq2Seq 這樣的任務，一般的 Deep Neural Network 是做不到的(雖然現在有了 Transformer 但目前先不考慮它)，其中一個原因是「順序」對於 Sequence 是有意義的，但是若我們將 Sequence 這樣的資料送進一般的 Deep Nerual Network，模型就只會依據每一次送進模型的資料給出相應的 Output，資料送進模型的順序並不會對 Output 有任何影響，因此要完成 Seq2Seq 這樣的任務就需要用到有循環結構的 RNN 模型。</p>
<p>另一方面，即便考慮了 RNN 模型，我們還是會面對到 Input Sequence: <span class="math">\(x_1, x_2, ..., x_S\)</span> 與 Target Sequence: <span class="math">\(y_1, y_2, ..., y_T\)</span> 長度不一致的情形。 因此在 <em>Sequence to Sequence Learning with Neural Networks</em> 中就提出以下兩個方案來完成 Seq2Seq 的任務：</p>
<ul>
<li>以 LSTM 做為神經網路模型的架構</li>
<li>將 Input Sequence: <span class="math">\(x_1, x_2, ..., x_S\)</span> 透過 RNN 先 map 到一個固定長度的 vector: <span class="math">\(v\)</span>，接著再將 <span class="math">\(v\)</span> 透過另一個 RNN map 到 Target Sequence: <span class="math">\(y_1, y_2, ..., y_T\)</span>，如此就能處理 Input Sequence 與 Target Sequence 長度不一致一情形</li>
</ul>
<p><img src="../images/intro_to_attention_02.png" alt="未顯示圖片" >
<center>
    <em>
        Fig. 2. Seq2Seq Model with RNN structure
    </em>
</center><br></p>
<p>Seq2Seq 模型的架構就如 <em>Fig. 2.</em> 所呈現，<BOS> 和 <EOS> 分別表示句子的開始(Begin of Sentence) 和結束(End of Sentence)，整體模型的架構可以拆解為兩個主要的子架構：<strong>Encoder</strong> 和 <strong>Decoder</strong>
* <strong>Encoder</strong>：其功用是把 Input Sequence 壓縮至一個固定長度的 <strong>Context Vector</strong> <span class="math">\(v\)</span>，因此可以把 <span class="math">\(v\)</span> 認定為它夾帶了整個 Input Sequence 必要的資訊。
* <strong>Decoder</strong>：以 Context Vector 及 \&lt;BOS> Token 做為 Decoder 的起始輸入，讓 Decoder 參照 Context Vector 等於是讓 Deocder 參照到 Input Sequence 的資訊，以及 \&lt;BOS> 只是單純讓 Decoder 開始根據 Context vector 所夾帶的資訊給出我們期望的 Target Sequence ，隨後 Decoder 的輸出一直到 \&lt;EOS> Token 出現為止。</p>
<p><img src="../images/intro_to_attention_03.png" alt="未顯示圖片" >
<center>
    <em>
        Fig. 3. Encoder-Decoder Structure
    </em>
</center><br></p>
<p>整個模型在做的事就是估計條件機率 <span class="math">\(p(y_1,...,y_T|x_1,...,x_S)\)</span>:</p>
<div class="math">\begin{align}
p(y_1,...,y_T|x_1,...,x_S)
    &amp;=p(y_1|v)\times 
    ...
    \times p(y_T|v,y_1,...,y_{T-1})\\
    &amp;=\prod_{t=1}^{T}p(y_t|v,y_1,...y_{t-1})
\end{align}</div>
<p>其中 <span class="math">\(p(y_t|v,y_1,...y_{t-1})\)</span> 取自於 Decoder 部份的 RNN 最後一層以 softmax 輸出的結果。</p>
<p>以上的 Encoder-Decoder 架構也可以有一些其它的變化，例如可以考慮將 Encoder 做出來的 Context vector 給 Decoder 每一個時間點的輸入，這樣子的做法出自於 <em>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em> 這篇論文，文中稱這樣的架構為 RNN Encoder-Decoder：</p>
<p><img src="../images/intro_to_attention_04.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 4. RNN Encoder-Decoder, 其中 Context vector </em><em>c</em><em> 皆會做為每一次 Hidden state 及 RNN output 的參考</em></p>
<p>由 <em>Fig. 4.</em> 不論是 Hidden state 或是 Decoder RNN 的輸出，皆會參考 Context vector <span class="math">\(c\)</span>：<br>
</p>
<div class="math">\begin{align}
    &amp;h_{t}=f(h_{t-1}, y_{t-1}, c)\\
    &amp;p(y_{t}|c,y_1,...,y_{t-1})=g(h_{t},y_{t-1}, c)
\end{align}</div>
<p>其中 <span class="math">\(f\)</span> 和 <span class="math">\(g\)</span> 為 nonlinear activation function 且 g 的輸出端接上 softmax 以讓輸出<span class="math">\(\mathbf{y}\)</span>表現出機率分佈。</p>
<h2>Basic Attention Mechanism</h2>
<p>以上的 Encoder-Decoder 架構皆是由一個統一的 Context vector(由 Encoder 吃完整個 input sequence 後所輸出的 Hidden state) 送給 Decoder 做為 input sequence 的資訊參考，這個做法所基於的一個前題是 Context vector <span class="math">\(\mathbf{c}\)</span> 必須包含了所有關於 input sequence 的必要資訊，但是實際上 Context vector 能不能包含必要的完整資訊傳送給 Decoder 其實是個問題，這樣的問題源自 RNN 本身的缺陷，也就是<strong>記憶能力的受限</strong>，尤其是當 input sequence 的長度一長時 sequence 前段的資訊就有可能被遺漏，LSTM 與 GRU 也是針對這個問題對 RNN 來做改良，但還是無法完全避免當 input sequence 長度一長時無法記錄完整資訊的這個問題。</p>
<p>基於以上的問題 <em>Neural Machine Translation by Jointly Learning to Align and Translate</em> 這篇論文提出了 Attention 的做法，讓 Decoder 在每一個時間點的輸出不會只參考固定的 Context vector <span class="math">\(c\)</span>,而是一個會變動的 Context vector <span class="math">\(c_t\)</span>，這個變動的 <span class="math">\(c_t\)</span> 會依據 Attention 的機制對 Encoder 每一個時間點的 Hidden state 給予不同程度的關注：</p>
<p><img src="../images/intro_to_attention_05.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 5. 結合 Attention 機制的 Encoder-Decoder 架構</em></p>
<p>這樣的機制釋放了 Encoder 要做出一個包含整個 input sequence 必要資訊的 Context vector 這樣的負擔，並且 Decoder 可以更有效的利用 input sequence 在每一個時間點帶來的資訊，這也使得在 MT 任務上如此的模型架構對於長句的翻譯表現得更好：</p>
<p><img src="../images/intro_to_attention_06.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 6. RNNsearch 為結合 Attention 機制的 Encoder-Decoder Model，RNNenc 為沒有使用 Attention 機制的 Encoder-Decoder Model，可以看出來長句子的翻譯上使用 Attention 的模型表現更好</em></p>
<h3>模型架構</h3>
<p>設 <span class="math">\(\mathbf{x}=(x_1, ...,x_S)\)</span> 和 <span class="math">\(\mathbf{y}=(y_1, ...,y_T)\)</span> 分別為 input sequence 和 target sequence, 模型一樣分為 Encoder 與 Decoder 兩個部分：</p>
<h4>Encoder</h4>
<p>Encoder 是一個 Bidirectional RNN, 將 input sequence <span class="math">\((x_1, ...,x_S)\)</span> 分別以順向與逆向送給 RNN 所作出來的 hidden states <span class="math">\(\overrightarrow{h_j}\)</span> 與 <span class="math">\(\overleftarrow{h_j}\)</span> 接併起來形成 <span class="math">\(h_j={[{\overrightarrow{h_j}}^T:{\overleftarrow{h_j}}^T]}^T\)</span>，論文中稱 <span class="math">\(h_j\)</span> 為 <span class="math">\(x_j\)</span> 的 annotation, 所以最後對於 input sequence <span class="math">\(\mathbf{x}\)</span> 我們會有 <span class="math">\(S\)</span> 個 annotations <span class="math">\((h_1, ..., h_S)\)</span>。 </p>
<h4>Decoder</h4>
<p>Decoder 要做的事就是估計條件機率 <span class="math">\(p(y_t|y_1,...,y_{t-1}, \mathbf{x})\)</span>, 可將其視為 decoder input <span class="math">\(y_{t-1}\)</span>, hidden state <span class="math">\(s_i\)</span>, 與 context vetcot <span class="math">\(c_i\)</span> 的一個函數 <span class="math">\(g\)</span>:
    </p>
<div class="math">\begin{align}
    p(y_t|y_1,...,y_{t-1}, \mathbf{x})=g(s_t, y_{t-1}, c_t)
    \end{align}</div>
<p>其中 <span class="math">\(s_t\)</span> 為在時間 <span class="math">\(t\)</span> 時的一個 RNN hidden state:
    </p>
<div class="math">\begin{align}
    s_t=f(s_{t-1}, y_{t-1}, c_t)
    \end{align}</div>
<p>另外要注意到這邊的 Context vector <span class="math">\(c_t\)</span>，它是隨時間變動的，由 <em>Fig. 5.</em> 可以了解 <span class="math">\(c_t\)</span> 會由 <span class="math">\(\alpha_{t,1},...,\alpha_{t,S}\)</span> 與 <span class="math">\(h_1, ..., h_S\)</span> 做 weighted sum 而得，<span class="math">\(\alpha_{t,1},...,\alpha_{t,S}\)</span> 即代表了在 <span class="math">\(t\)</span> 這個時間點 decode 時所參考 encoder 各時間點的 hidden state 的比重，而這些 <span class="math">\(\alpha_{t,j}\)</span> 就是由 <strong>Attention mechanism</strong> 計算而得，計算的方式並不複雜：
1. 以一個 score function <span class="math">\(\mathrm{score}\)</span> 將上一個時間點的 hidden state <span class="math">\(s_{t-1}\)</span> 與在 Encoder 當中每個時間點的 annotation <span class="math">\(h_j, \, j=1,...,S\)</span> 取得兩者之間的分數 <span class="math">\(\mathrm{score}(s_{t-1},h_j), \, j=1,...,S\)</span>, 這個 score function 可以是一個 feedforward network with one hidden layer 論文中的定義如下：
    </p>
<div class="math">\begin{align}
    \mathrm{score}(s_{t-1},h_j)=v_a^T\tanh(W_as_{t-1}+U_ah_j)
    \end{align}</div>
<p>論文中稱這個 score function 為 alignment model。</p>
<ol>
<li>接著再將每一個 <span class="math">\(\mathrm{score}(s_{t-1},h_j)\)</span> 做標準化(normalization)，即取得 <span class="math">\(\alpha_{t,j}\)</span>:
    <div class="math">\begin{align}
    \alpha_{t,j}=\frac{\exp(\mathrm{score}(s_{t-1},h_j))}{\sum\limits_{k=1}^S\exp(\mathrm{score}(s_{t-1},h_k))}
    \end{align}</div>
</li>
</ol>
<p>最後，Context vector <span class="math">\(c_t=\sum\limits_{j=1}^S\alpha_{t,j}h_j\)</span></p>
<h3>結果呈現</h3>
<p>模型實作是以英文翻譯法文為目標，<em>Fig. 7.</em> 中的每一個 pixel 表示 <span class="math">\(\alpha_{t,j}\)</span> 的值(第 j 個 target word(法文)對第 t 個 sorce word(英文) 的 annotation 的權重 <span class="math">\(\alpha\)</span> 值，以灰階表示 0:black, 1:white)<br>
<img src="../images/intro_to_attention_07.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 7. 英文對於法文的 attention 結果，英文是 source 法文是模型翻譯後的結果</em></p>
<h2>Attention applied in QA Problem</h2>
<p>接下來可以再看看另一個 Attention 的例子，這個例子是把 Attention mechanism 加入 QA 的模型中，從而模型達到了更好的表現。任務是給定一個問題 <span class="math">\(q\)</span> 和對於問題 <span class="math">\(q\)</span> 的一池候選答案 <span class="math">\(P = \{a_1, a_2,...,a_p \}\)</span> ，我們要從候選答案中找出正確回答問題 <span class="math">\(q\)</span> 的答案 <span class="math">\(a \in P\)</span>。</p>
<p>對於這樣的問題一種簡單的模型架構做法是可以設計兩條 NN(Neural Network) 路徑，一條作為輸入問題 <span class="math">\(q\)</span> 使用，另一條作為輸入候選答案 <span class="math">\(a\)</span> 使用，當 <span class="math">\(q\)</span> 和 <span class="math">\(a\)</span> 經過各自的 NN 路抽取出各自的 representation <span class="math">\(r_q\)</span> 和 <span class="math">\(r_a\)</span> 後，可以再介由適當的 metric 來比對 <span class="math">\(r_q\)</span> 和 <span class="math">\(r_a\)</span> 的相似度，例如 cosine-similarity ，而正確答案就以相似度與 <span class="math">\(q\)</span> 最高的 <span class="math">\(a\)</span> 來認定。</p>
<p><img src="../images/intro_to_attention_08.png" alt="未顯示圖片" height="60%" width="60%"></p>
<p><em>Fig. 8. 模型內部可採用 CNN 或 LSTM 來取得 representation</em></p>
<h3>模型架構</h3>
<h4>基本模型架構</h4>
<p>以下我們參照 <em>Fig. 8.</em> 來解釋模型架構，首先給定我們的問題 <span class="math">\(q\)</span> 與 候選答案 <span class="math">\(a\)</span>，<span class="math">\(q\)</span> 和 <span class="math">\(a\)</span> 各自包合了 <span class="math">\(M\)</span> 和 <span class="math">\(L\)</span> 個 tokens(也就是可以理解為 <span class="math">\(q\)</span> 和 <span class="math">\(a\)</span> 兩個敍述各自包含了多少個字)：</p>
<ol>
<li>
<p><span class="math">\(q\)</span> 和 <span class="math">\(a\)</span> 會先通過各自的 word embedding layer, 每一個 token 會變成一個 word vector ，以 <span class="math">\(q^{emb}=\{r^{w_1}, r^{w_2}, ..., r^{w_M}\}\)</span> 和 <span class="math">\(a^{emb}=\{r^{w_1}, r^{w_2}, ..., r^{w_L}\}\)</span> 表示，其中每個 <span class="math">\(r^w \in \mathbb{R}^d\)</span>。</p>
</li>
<li>
<p>接著可以使用 CNN 或 LSTM 來處理 word embedding 後的序列:</p>
<ul>
<li>
<p>我們先以使用 CNN 的方式作為說明, 以 <span class="math">\(q^{emb}=\{r^{w_1}, r^{w_2}, ..., r^{w_M}\}\)</span> 中的每一個 word embedding <span class="math">\(r^{w_m}\)</span> 為中心將前後總共 <span class="math">\(k\)</span> 個 word embedding 接併起來，形成 <span class="math">\(z_m \in \mathbb{R}^{dk}\)</span>(所以 <span class="math">\(k\)</span> 就是 CNN 的 filter size)，定義矩陣 <span class="math">\(Z^q=[z_1,z_2,...,z_M]\)</span>，我們也可設定 convolution 的 filter 數量，我們定為 <span class="math">\(c\)</span>，所以 CNN 的運算即為：
<div class="math">\begin{align}
Q=W^1Z^q+b^1
\end{align}</div>
其中 <span class="math">\(W^1\)</span> 與 <span class="math">\(b^1\)</span> 為需要學習的參數，所得到的矩陣 <span class="math">\(Q \in \mathbb{R}^{c \times M}\)</span> 的第 <span class="math">\(m\)</span> 個 column 包含了透過 context window(即 filter)在第 <span class="math">\(m\)</span> 個字周圍萃取出來的特徵。</p>
</li>
<li>
<p>同樣的做法也可用在 <span class="math">\(a^{emb}=\{r^{w_1}, r^{w_2}, ..., r^{w_L}\}\)</span> 上，使用相同的 NN 參數 <span class="math">\(W^1\)</span> 和 <span class="math">\(b^1\)</span> 計算矩陣 <span class="math">\(A \in \mathbb{R}^{c \times L}\)</span>:
<div class="math">\begin{align}
A=W^1Z^a+b^1
\end{align}</div>
</p>
</li>
<li>
<p>LSTM 的做法也是類似，不過是透過雙向的 LSTM (biLSTM) 將順向與逆向每一個 time step 的 hiddem states 接併起來形成 <span class="math">\(h_t = \overrightarrow{h_t}||\overleftarrow{h_t}\)</span>，最後 <span class="math">\(Q\)</span> 與 <span class="math">\(A\)</span> 的 column 就是由各自 <span class="math">\(q\)</span> 與 <span class="math">\(a\)</span> 作為輸入所得的 hidden states 所組成的矩陣。</p>
</li>
</ul>
</li>
<li>
<p>在取得矩陣 <span class="math">\(Q\)</span> 與 <span class="math">\(A\)</span> 之後，我們可以對各別矩陣採取 column-wise max-pooling，隨後再套用 <span class="math">\(\tanh\)</span> 作為 activation function 以取得 <span class="math">\(q\)</span> 與 <span class="math">\(a\)</span> 的 representations <span class="math">\(r^q\)</span> 與 <span class="math">\(r^a\)</span>:
    <div class="math">\begin{align}
    r^q = \tanh(\max\limits_{1&lt;m&lt;M}[Q_{j,m}])\\
    r^a = \tanh(\max\limits_{1&lt;m&lt;L}[A_{j,l}])
    \end{align}</div>
</p>
</li>
<li>
<p>最後再取得 <span class="math">\(r^q\)</span> 與 <span class="math">\(r^a\)</span> 的 cosine-similarity，作為 <span class="math">\(q\)</span> 與 <span class="math">\(a\)</span> 比對的分數:
    <div class="math">\begin{align}
    s(q,a)=\frac{r^q \cdot r^q}{||r^q||||r^a||}
    \end{align}</div>
</p>
</li>
</ol>
<p>在模型訓練上採用 triplet-loss:
    </p>
<div class="math">\begin{align}
    L = \max\{0, m-s(q,a^+)+s(q,a^-)\}
    \end{align}</div>
<p>其中 <span class="math">\(a^+\)</span> 代表對於問題 <span class="math">\(q\)</span> 而言正確的答案，<span class="math">\(a^-\)</span> 代表錯誤的答案，<span class="math">\(m\)</span> 的意思是 margin，整個 loss function 的用意是讓 <span class="math">\(s(q,a^+)\)</span> 與 <span class="math">\(s(q,a^-)\)</span> 的差距高過一個 margin <span class="math">\(m\)</span> 以上，以至於模型在所謂的 latent space 中能更容易區分 <span class="math">\(a^+\)</span> 與 <span class="math">\(a^-\)</span>。</p>
<h4>Attention Pooling</h4>
<p>以上所談的模型架構是沒加入 Attention mechanism 以前的基本做法，此篇論文提出 Attentive Pooling(AP) 的方法，此方法是一種 two-way Attention mechanism, 這裡的 two-way 應該是兩個方向的意思，表示這樣的機制不僅會讓 <span class="math">\(q\)</span> attend to <span class="math">\(a\)</span> 同時也會讓 <span class="math">\(a\)</span> attend to <span class="math">\(q\)</span>. </p>
<p>由 <em>Fig. 9.</em> AP 的機制是在原先的模型架構上取得 <span class="math">\(Q\)</span> 和 <span class="math">\(A\)</span> 矩陣之後計算
    </p>
<div class="math">\begin{align}
    G=\tanh(Q^TUA)
    \end{align}</div>
<p>其中 <span class="math">\(U \in \mathbb{R}^{c \times c}\)</span> 是需要透過 NN 學習的參數。</p>
<p><img src="../images/intro_to_attention_09.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 9. Attentive Pooling Networks</em></p>
<p>這邊我們可以來看看 <span class="math">\(Q^TUA\)</span> 的運算：
設 
    </p>
<div class="math">\begin{align}
    Q^T=
    \left[
    \begin{array}{ccc}
    -q_1^T- \\
    -q_2^T- \\
    \vdots \\
    -q_M^T-
    \end{array}
    \right], \,
    A=
    \left[
    \begin{array}{cccc}
    | &amp; | &amp; &amp; |\\
    a_1 &amp; a_2 &amp;\dots&amp; a_L \\
    | &amp; | &amp; &amp; |\\
    \end{array}
    \right]
    \end{align}</div>
<p>其中 <span class="math">\(q_i^T \in \mathbb{R}^{1 \times c},
    a_i \in \mathbb{R}^{c \times 1}\)</span>。
    </p>
<div class="math">\begin{align}
    Q^TUA
    &amp;=Q^T
    \left[
    \begin{array}{cccc}
    | &amp; | &amp; &amp; |\\
    Ua_1 &amp; Ua_2 &amp;\dots&amp; Ua_L \\
    | &amp; | &amp; &amp; |\\
    \end{array}
    \right] \\
    &amp;=
    \left[
    \begin{array}{cccc}
    q_1^TUa_1 &amp; q_1^TUa_2 &amp; \dots &amp; q_1^TUa_L \\
    q_2^TUa_1 &amp; q_2^TUa_2 &amp; \dots &amp; q_2^TUa_L \\
    \vdots &amp; \vdots &amp; \dots &amp; \vdots \\
    q_M^TUa_1 &amp; q_M^TUa_2 &amp; \dots &amp; q_M^TUa_L
    \end{array}
    \right]
    \end{align}</div>
<p>由以上的計算，我們看到每一列(row)是由一個特定的 <span class="math">\(q_i\)</span> 與每一個 <span class="math">\(Ua_j, \, j=1,2,...,L\)</span> 的內積，所以第 <span class="math">\(i\)</span> 列的這些內積值 <span class="math">\(q_i^TUa_1, \, q_i^TUa_2, \,...,\, q_i^TUa_L\)</span> 可視為特定的 <span class="math">\(q_i\)</span> 對個別 <span class="math">\(a_j, \, j=1,2,...,L\)</span> 的關注分數。同理，若是由每一行(column)來看，每一行是由特定的 <span class="math">\(a_j\)</span> 與每一個 <span class="math">\(U^Tq_i, \, i=1,2,...,M\)</span> 的內積，因此第 <span class="math">\(j\)</span> 行的內積值 <span class="math">\((U^Tq_1)^Ta_j, \, (U^Tq_2)^Ta_j, \,..., \, (U^Tq_M)^Ta_j\)</span> 可視為特定的 <span class="math">\(a_j\)</span> 對個別 <span class="math">\(q_i, \, i=1,2,...,M\)</span> 的關注分數。我們計算 <span class="math">\(G\)</span> 如下：
    </p>
<div class="math">\begin{align}
    G=\tanh(Q^TUA)
    \end{align}</div>
<p>由以上討論，若對 <span class="math">\(G\)</span> 做 column-wise max-pooling，我們會得到 <span class="math">\(g^q\)</span>:
    </p>
<div class="math">\begin{align}
    [g^q]_i=\max\limits_{1 \leq l \leq L}G_{i,l}
    \end{align}</div>
<p>其意義表示在問題 <span class="math">\(q\)</span> 的第 <span class="math">\(i\)</span> 個字附近對於整個答案 <span class="math">\(a\)</span> 的 importance score。同樣地，若對 <span class="math">\(G\)</span> 做 row-wise max-pooling，我們會得到 <span class="math">\(g^a\)</span>:
    </p>
<div class="math">\begin{align}
    [g^a]_j=\max\limits_{1 \leq m \leq M}G_{m,j}
    \end{align}</div>
<p>其意義表示在答案 <span class="math">\(a\)</span> 的第 <span class="math">\(j\)</span> 個字附近對於整個問題 <span class="math">\(q\)</span> 的 importance score。</p>
<p>經過前面的計算分別算出 importance scores <span class="math">\(g^q\)</span> 和 <span class="math">\(g^a\)</span> 之後，接下來就與 <strong>Basic Attention Mechanism</strong> 中一樣做 normalization:
    </p>
<div class="math">\begin{align}
    &amp;[\sigma^q]_i=\frac{[g^q]_i}{\sum\limits_{m=1}^M[g^q]_m}\\
    &amp;[\sigma^a]_j=\frac{[g^a]_j}{\sum\limits_{l=1}^L[g^a]_l}
    \end{align}</div>
<p>得到 normalized importance scores <span class="math">\([\sigma^q]_i\)</span> 和 <span class="math">\([\sigma^a]_j\)</span> 之後，我們要做的最後一步就是將每個 score 作為對應到 <span class="math">\(q_i\)</span> 和 <span class="math">\(a_j\)</span> 的權重然後計算其加權平均, 即得到最終的 representations <span class="math">\(r^q\)</span> 和 <span class="math">\(r^a\)</span>：
    </p>
<div class="math">\begin{align}
    r^q&amp;=[\sigma^q]_1q_1+[\sigma^q]_2q_2+...+[\sigma^q]_Mq_M\\
    &amp;=Q\sigma^q
    \end{align}</div>
<p>
同理，<span class="math">\(r^a=A\sigma^a\)</span>。</p>
<h3>結果</h3>
<p>在加入了 AP 機制以後, 由 <em>Fig. 10.</em> 可以看到不論是 AP-CNN 或是 AP-BILSTM 表現不僅比原來的 QA-CNN 與 QA-BILSTM 好，也贏過了當時的 SOTA(state-of-the-art) (Feng et al.,2015) 與 (Tan et al., 2015)。</p>
<p><img src="../images/intro_to_attention_10.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 10.</em> 不同模型在 InsuranceQA dataset 上的準確度</p>
<p>不僅僅是準確度的提升，由 <em>Fig. 11.</em> 加入 Attention 機制後的 AP-CNN 所需要的參數量比起 QA-CNN 來得更少。</p>
<p><img src="../images/intro_to_attention_11.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 11.</em> 各個模型的超參數設定</p>
<p>同時也可以看到一般而言準確度會隨著 Answer 的長度逐漸降低，但是 AP-CNN 下降的幅度卻沒有 QA-CNN 來得多。
<img src="../images/intro_to_attention_12.png" alt="未顯示圖片" height="60%" width="60%" ></p>
<p><em>Fig. 12.</em> 隨著 Answer 長度增加模型的準確度表現</p>
<p>最後由 <em>Fig. 13.</em> 也可以看到對於 AP-CNN 正確回答的 Question 與 Answer ，Attention 對 Question 與 Ansewer 各部位所關注的程度, 其中顏色愈深表示關注的程度愈大。
<img src="../images/intro_to_attention_13.png" alt="未顯示圖片"  height="60%" width="60%"></p>
<p><em>Fig. 13.</em> Attention 關注度的 heat map</p>
<h2>References</h2>
<ol>
<li>[Sutskever, et al. 2014] <a href="https://arxiv.org/abs/1409.3215"><strong><em>Sequence to Sequence Learning with Neural Networks</em></strong></a></li>
<li>[KyungHyun Cho, et al. 2014] <a href="https://arxiv.org/abs/1406.1078"><strong><em>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em></strong></a></li>
<li>[Dzmitry Bahdanau, et al. 2015] <a href="https://arxiv.org/abs/1409.0473"><strong><em>Neural Machine Translation by Jointly Learning to Align and Translate</em></strong></a></li>
<li>[Cicero dos Santos, et al. 2016] <a href="https://arxiv.org/abs/1602.03609"><strong><em>Attentive Pooling Networks</em></strong></a></li>
<li>[蘑菇先生學習記]　<a href="http://xtf615.com/2019/01/06/attention/"><strong><em>Attention in Deep Learning</em></strong></a></li>
<li>[Lil'Log]　<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"><strong><em>Attention? Attention!</em></strong></a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>